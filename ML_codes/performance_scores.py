import numpy as np
import matplotlib.pyplot as plt
from scipy.stats import norm
from sklearn.metrics import (
    accuracy_score, precision_score, recall_score,
    roc_curve, auc, f1_score, confusion_matrix
)


def compute_binomial_error(metric_value, n_samples, confidence_level):

    """

    Estimate the error margin for a binomial metric using the normal approximation.

    This function calculates the error associated with metrics such as accuracy, precision,
    or recall by approximating the binomial distribution with a normal distribution.

    Parameters
    ----------
    metric_value : float
        The value of the metric (between 0 and 1), such as accuracy, precision, or recall.

    n_samples : int
        The number of independent observations (e.g., test set size).

    confidence_level : float
        The confidence level for the error estimation (e.g., 0.683 for 68.3% confidence, 0.95 for 95% confidence).

    Returns
    -------
    float
        The estimated error margin based on the z-score and binomial variance.

    Notes
    -----
    - This approximation assumes a sufficiently large number of samples (n), satisfying the condition: n * p and n * (1 - p) ≥ 5.
    - The z-score is derived from the standard normal distribution for the specified confidence level.

    Formula
    -------
    The error is calculated using the formula:
        error ≈ z * sqrt(p * (1 - p) / n)
    where:
    - z is the z-score for the specified confidence level,
    - p is the metric value,
    - n is the number of samples.

    Examples
    --------
    >>> from scipy.stats import norm
    >>> compute_binomial_error(0.85, 100, 0.95)
    0.0647  # Approximate value

    """


    z = norm.ppf((1 + confidence_level) / 2.0)
    return z * np.sqrt((metric_value * (1 - metric_value)) / n_samples)

def evaluate_model_performance(y_true, y_pred, y_proba, confidence_level=0.683):

    """

    Evaluate the performance of a classification model using various metrics.

    This function computes common evaluation metrics such as accuracy, precision, recall,
    and F1-score based on the provided true and predicted labels. It returns a dictionary
    containing the results for the specified metric.

    Parameters
    ----------
    y_true : list or array-like
        The ground truth (actual labels).

    y_pred : list or array-like
        The predicted labels generated by the model.

    metric : str, optional
        The evaluation metric to compute. Default is 'accuracy'.
        Supported metrics include:
        - "accuracy"
        - "precision"
        - "recall"
        - "f1"

    Returns
    -------
    dict
        A dictionary containing the evaluation results with keys corresponding to
        the metric names and values as the computed scores.

    Examples
    --------
    >>> from my_module import evaluate_model_performance
    >>> y_true = [0, 1, 1, 0]
    >>> y_pred = [0, 1, 0, 0]
    >>> results = evaluate_model_performance(y_true, y_pred, metric="f1")
    >>> print(results)
    {'f1': 0.6667}
    
    """

    # Ensure probabilities are 1D for the positive class
    if y_proba.ndim == 2:
        y_proba = y_proba[:, 1]

    # Compute core metrics
    accuracy = accuracy_score(y_true, y_pred,)
    precision = precision_score(y_true, y_pred, zero_division='warn')
    recall = recall_score(y_true, y_pred, zero_division='warn')
    f1 = f1_score(y_true, y_pred, zero_division='warn')

    # Confusion matrix for specificity
    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel() # ravel() flattens the matrix into a 1D array
    specificity = tn / (tn + fp) if (tn + fp) > 0 else 0.0

    # Estimate errors
    n = len(y_true)
    acc_err = compute_binomial_error(accuracy, n, confidence_level)
    prec_err = compute_binomial_error(precision, n, confidence_level)
    rec_err = compute_binomial_error(recall, n, confidence_level)
    f1_err = compute_binomial_error(f1, n, confidence_level)
    spec_err = compute_binomial_error(specificity, n, confidence_level)


    # ROC and AUC computation
    fpr, tpr, _ = roc_curve(y_true, y_proba, pos_label= 1)
    roc_auc = auc(fpr, tpr)

    # AUC error estimation based on classical Hanley & McNeil (1982) approach
    # which is a normal approximation
    # for the variance of the AUC statistic.
    n1 = np.sum(y_true == 1)
    n0 = np.sum(y_true == 0)

    q1 = roc_auc / (2 - roc_auc)
    q2 = 2 * roc_auc ** 2 / (1 + roc_auc)

    z = norm.ppf((1 + confidence_level) / 2.0)
    auc_err = z * np.sqrt(
        (roc_auc * (1 - roc_auc) +
         (n1 - 1) * (q1 - roc_auc ** 2) +
         (n0 - 1) * (q2 - roc_auc ** 2)) / (n1 * n0)
    )

    # Plot ROC Curve
    #plt.style.use('seaborn-v0_8-darkgrid')
    plt.figure(figsize=(8, 6))
    plt.plot(fpr, tpr, color='#FF6F61', lw=3,
             label=f'ROC curve (AUC = {roc_auc:.2f} ± {auc_err:.2f})')
    plt.plot([0, 1], [0, 1], color='gray', linestyle='--', lw=2)
    plt.xlim([-0.01, 1.01])
    plt.ylim([-0.01, 1.05])
    plt.xlabel('False Positive Rate', fontsize=13)
    plt.ylabel('True Positive Rate', fontsize=13)
    plt.title('ROC Curve', fontsize=15, fontweight='bold')
    plt.legend(loc="lower right", fontsize=11)
    plt.grid(True, linestyle='--', alpha=0.5)
    plt.tight_layout()
    plt.show()

    # --- Bar Plot with Error Bars ---
    metrics = ['Accuracy', 'Precision', 'Recall', 'F1-score', 'Specificity', 'AUC']
    values = [accuracy, precision, recall, f1, specificity, roc_auc]
    errors = [acc_err, prec_err, rec_err, f1_err, spec_err, auc_err]
    colors = ['#4C72B0', '#55A868', '#C44E52', '#8172B2', '#CCB974', '#64B5CD']

    fig, ax = plt.subplots(figsize=(10, 6))
    bars = ax.bar(metrics, values, yerr=errors, capsize=10,
                  color=colors, edgecolor='black', linewidth=1.5)

    ax.set_ylim(0, 1.1)
    ax.set_ylabel("Score", fontsize=13)
    ax.set_title("Classification Metrics with Confidence Errors", fontsize=15, fontweight='bold')
    ax.grid(axis='y', linestyle='--', alpha=0.6)
    ax.bar_label(bars, fmt="%.2f", padding=4, fontsize=11)
    plt.xticks(fontsize=11)
    plt.yticks(fontsize=11)
    plt.tight_layout()
    plt.show()

    # Print results
    print(f"Accuracy:  {accuracy:.2f} ± {acc_err:.2f}")
    print(f"Precision: {precision:.2f} ± {prec_err:.2f}")
    print(f"Recall:    {recall:.2f} ± {rec_err:.2f}")
    print(f"F1-score:    {f1:.2f} ± {f1_err:.2f}")
    print(f"Specificity: {specificity:.2f} ± {spec_err:.2f}")
    print(f"AUC:       {roc_auc:.2f} ± {auc_err:.2f}")

    return {
        'Accuracy': accuracy, 'Accuracy_error': acc_err,
        'Precision': precision, 'Precision_error': prec_err,
        'Recall': recall, 'Recall_error': rec_err,
        'F1-score': f1, 'F1-score_error': f1_err,
        'Specificity': specificity, 'Specificity_error': spec_err,
        'AUC': roc_auc, 'AUC_error': auc_err
    }




